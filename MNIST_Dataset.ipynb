{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyNVi/sE6Twg2Mpd5qsnWJV6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Keras models can be functional or sequential apis\n","\n","Sequential model:\n","* simple,easy to use way to define models that consist of linear stacks of layers, layers are added to model in sequential order\n","*   output of each layer is fed as input to the next\n","\n","Functional Model:\n","*  used to define more complex deep learning models with multiple inputs and outputs, or ones that share layers\n","*   you can also have branches, where instead of being fed to the next layer only, you can branch off\n","* non linear flows\n","* layers are defined as seperate objects, inputs,outputs are explicilty defined.\n","\n","\n","Use the mnist dataset to explore fucntional and sequential models\n","\n"],"metadata":{"id":"hIm0HkYkd8ZD"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cl_oHDIFd5FH","executionInfo":{"status":"ok","timestamp":1695934367225,"user_tz":240,"elapsed":4523,"user":{"displayName":"tj toor","userId":"12772828922137278049"}},"outputId":"207f032d-e46b-4ae4-8648-c28c01fbb36e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 1s 0us/step\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.datasets import mnist\n","\n","(xtrain,ytrain),(xtest,ytest)=mnist.load_data()"]},{"cell_type":"code","source":["print(xtrain.shape)\n","print(ytrain.shape)\n","\n","# we are going to send the images to a NN, so lets flatten the values and also normalize between 0 and 1\n","xtrain=xtrain.reshape(-1,28*28).astype(\"float32\")/255.0\n","xtest=xtest.reshape(-1,28*28).astype(\"float32\")/255.0\n","\n","\n","print(xtrain.shape)\n","print(xtest.shape)\n","\n","# you dont need to convert these numpy arrays, internally they will be converted to tensors"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdLKMutEeuSS","executionInfo":{"status":"ok","timestamp":1695934367226,"user_tz":240,"elapsed":10,"user":{"displayName":"tj toor","userId":"12772828922137278049"}},"outputId":"4230e0c2-29ee-439e-e10b-d6731a3cb98d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28)\n","(60000,)\n","(60000, 784)\n","(10000, 784)\n"]}]},{"cell_type":"code","source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","# Create a Sequential API -> very conveinent but not flexible (one input can map to one output)\n","# layers.dense is basically just a layer that has the inputs from previous layer, ie every node here is connected to every input from previous layer\n","# you can have as many of these as you would like\n","\n","#in the model part you are basically defining the layers\n","model = keras.Sequential(\n","    [\n","        layers.Dense(512,activation='relu'),\n","        layers.Dense(256,activation='relu'),\n","        #below is the output layer, you can say here the softmax function\n","        layers.Dense(10)\n","\n","    ]\n","\n",")\n","\n","# in the mode compile you are stating the optimizer and loss function and stuff\n","\n","model.compile(\n","    #below if you use from_logits=True, you have to put this here, if you didnt put the\n","    #softmax in the output above, this will first send through a softmax and then to the sparsecategorical......\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=keras.optimizers.Adam(lr=0.001),\n","    metrics=[\"accuracy\"]\n",")\n","\n","#specify the trianing stuff\n","model.fit(xtrain,ytrain,batch_size=32,epochs=5,verbose=2)\n","\n","model.evaluate(xtest,ytest,batch_size=32,verbose=2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MThXajwge5Dh","executionInfo":{"status":"ok","timestamp":1695934396754,"user_tz":240,"elapsed":29535,"user":{"displayName":"tj toor","userId":"12772828922137278049"}},"outputId":"3c8cf93c-361e-4a6b-95b8-bfef75a333be"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1875/1875 - 9s - loss: 0.1835 - accuracy: 0.9439 - 9s/epoch - 5ms/step\n","Epoch 2/5\n","1875/1875 - 4s - loss: 0.0796 - accuracy: 0.9747 - 4s/epoch - 2ms/step\n","Epoch 3/5\n","1875/1875 - 4s - loss: 0.0538 - accuracy: 0.9825 - 4s/epoch - 2ms/step\n","Epoch 4/5\n","1875/1875 - 4s - loss: 0.0410 - accuracy: 0.9869 - 4s/epoch - 2ms/step\n","Epoch 5/5\n","1875/1875 - 4s - loss: 0.0341 - accuracy: 0.9891 - 4s/epoch - 2ms/step\n","313/313 - 1s - loss: 0.0950 - accuracy: 0.9748 - 640ms/epoch - 2ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.09504590183496475, 0.9747999906539917]"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["Batch Size:\n","\n","-> basically batch size is the number of samples to be used for each iteration, so like if it is 32, you want 32 samples to train before the paramters are updated and you go to iteration 2\n","\n","BatchMode: batch size is equal to total dataset, so the iterations and epochs are equivalent\n","\n","mini batch mode: batch size is greater than 1 but less than the total dataset size\n","\n","stochastic mode: the batch size is equal to 1, so paramters updated after every sample\n","\n","\n","-> iteration: an iteration is just when the weights are updated\n","\n","-> epochs: basically when you train the dataset once, your gradient descent takes very small step, so one iteration of the current training data might not be enough, so you train the data through multiple epochs\n","\n"],"metadata":{"id":"91b3WyjskjFg"}},{"cell_type":"code","source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","model = keras.Sequential(\n","    [\n","        keras.Input(shape=(28*28)),\n","        layers.Dense(512,activation='relu'),\n","        layers.Dense(256,activation='relu'),\n","        #below is the output layer, you can say here the softmax function\n","        layers.Dense(10)\n","\n","    ]\n","\n",")\n","\n","print(model.summary())\n","#if you didnt add that input lin in the model above, you would not be able to see the model summary printed until after you compile\n","\n","\n","# in the mode compile you are stating the optimizer and loss function and stuff\n","model.compile(\n","    #below if you use from_logits=True, you have to put this here, if you didnt put the\n","    #softmax in the output above, this will first send through a softmax and then to the sparsecategorical......\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=keras.optimizers.Adam(lr=0.001),\n","    metrics=[\"accuracy\"]\n",")\n","\n","#specify the trianing stuff\n","model.fit(xtrain,ytrain,batch_size=32,epochs=5,verbose=2)\n","\n","model.evaluate(xtest,ytest,batch_size=32,verbose=2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6tohYxLsjWDX","executionInfo":{"status":"ok","timestamp":1695935205641,"user_tz":240,"elapsed":21708,"user":{"displayName":"tj toor","userId":"12772828922137278049"}},"outputId":"94258e9d-6c99-4b5a-a898-d18e526ca719"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_3 (Dense)             (None, 512)               401920    \n","                                                                 \n"," dense_4 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_5 (Dense)             (None, 10)                2570      \n","                                                                 \n","=================================================================\n","Total params: 535818 (2.04 MB)\n","Trainable params: 535818 (2.04 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"output_type":"stream","name":"stdout","text":["None\n","Epoch 1/5\n","1875/1875 - 5s - loss: 0.1868 - accuracy: 0.9427 - 5s/epoch - 3ms/step\n","Epoch 2/5\n","1875/1875 - 4s - loss: 0.0799 - accuracy: 0.9749 - 4s/epoch - 2ms/step\n","Epoch 3/5\n","1875/1875 - 4s - loss: 0.0543 - accuracy: 0.9830 - 4s/epoch - 2ms/step\n","Epoch 4/5\n","1875/1875 - 4s - loss: 0.0414 - accuracy: 0.9866 - 4s/epoch - 2ms/step\n","Epoch 5/5\n","1875/1875 - 4s - loss: 0.0335 - accuracy: 0.9894 - 4s/epoch - 2ms/step\n","313/313 - 1s - loss: 0.0879 - accuracy: 0.9774 - 598ms/epoch - 2ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.08786247670650482, 0.977400004863739]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["#Functional Apis\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","inputs = keras.Input(shape=(28*28))\n","x=layers.Dense(512,activation='relu', name='first_layer')(inputs)\n","x=layers.Dense(256,activation='relu')(x)\n","outputs=layers.Dense(10,activation='softmax')(x)\n","model=keras.Model(inputs=inputs,outputs=outputs)\n","\n","model.compile(\n","    #below if you use from_logits=True, you have to put this here, if you didnt put the\n","    #softmax in the output above, this will first send through a softmax and then to the sparsecategorical......\n","\n","    #becasue we made the softmax above, you can just set the logits=false or just remove it\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","    optimizer=keras.optimizers.Adam(lr=0.001),\n","    metrics=[\"accuracy\"]\n",")\n","\n","#specify the trianing stuff\n","model.fit(xtrain,ytrain,batch_size=32,epochs=5,verbose=2)\n","\n","model.evaluate(xtest,ytest,batch_size=32,verbose=2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ef8v8wyPnTdi","executionInfo":{"status":"ok","timestamp":1695936167224,"user_tz":240,"elapsed":21320,"user":{"displayName":"tj toor","userId":"12772828922137278049"}},"outputId":"c897c9c2-f52e-488d-8464-9cc426036ccd"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1875/1875 - 5s - loss: 0.1872 - accuracy: 0.9428 - 5s/epoch - 3ms/step\n","Epoch 2/5\n","1875/1875 - 4s - loss: 0.0804 - accuracy: 0.9749 - 4s/epoch - 2ms/step\n","Epoch 3/5\n","1875/1875 - 4s - loss: 0.0557 - accuracy: 0.9821 - 4s/epoch - 2ms/step\n","Epoch 4/5\n","1875/1875 - 4s - loss: 0.0428 - accuracy: 0.9863 - 4s/epoch - 2ms/step\n","Epoch 5/5\n","1875/1875 - 4s - loss: 0.0323 - accuracy: 0.9895 - 4s/epoch - 2ms/step\n","313/313 - 1s - loss: 0.1236 - accuracy: 0.9675 - 622ms/epoch - 2ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.1235736608505249, 0.9674999713897705]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# how to get outputs from certain layers -- works for both the sequential and functional api\n","#model.layers[-1] means last layer, [-2] means second last, etc\n","\n","#the below overwrites the aove model\n","model=keras.Model(inputs=model.inputs,outputs=[model.layers[-2].output])\n","\n","# you could also replace the [-2] with model.get_layer('layer_name).output\n","\n","feature=model.predict(xtrain)\n","print(feature.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y8NnFil5qvX_","executionInfo":{"status":"ok","timestamp":1695936467079,"user_tz":240,"elapsed":3116,"user":{"displayName":"tj toor","userId":"12772828922137278049"}},"outputId":"3ee76c40-97c7-4264-fda4-715666cfe255"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["1875/1875 [==============================] - 2s 1ms/step\n","(60000, 512)\n"]}]},{"cell_type":"code","source":["# you could also get the output from every layer, will be in different matricies\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","model=keras.Model(inputs=model.inputs,outputs=[layer.output for layer in model.layers])\n","\n","features=model.predict(xtrain)\n","\n","for feature in features:\n","  print(features.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":227},"id":"TCB9zpAqrtev","executionInfo":{"status":"error","timestamp":1695936682528,"user_tz":240,"elapsed":5177,"user":{"displayName":"tj toor","userId":"12772828922137278049"}},"outputId":"986b7019-4d45-4a31-cfd0-8e5777b368e0"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["1875/1875 [==============================] - 3s 1ms/step\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-0e956353af2c>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"cgVTLX77sbJT"},"execution_count":null,"outputs":[]}]}