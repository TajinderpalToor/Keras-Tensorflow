{"cells":[{"cell_type":"markdown","metadata":{"id":"jBqUI7BXUZNh"},"source":["We go over making the cnn from scratch using the functional and sequantial api\n","\n","added batch normalization to it\n","https://www.youtube.com/watch?v=DtEq44FTPM4&ab_channel=CodeEmporium\n","\n","https://www.youtube.com/watch?v=nUUqwaxLnWs&t=4s&ab_channel=DeepLearningAI\n","\n","Batch Normilization Paper -> https://arxiv.org/abs/1502.03167\n","\n","Batch Normilization and how it helps optimization:\n","https://arxiv.org/abs/1805.11604\n","\n","Batch Normilization:\n","\n","Increases speed of training:\n","\n","* so if one variable has a small range, like height which will go from 0-2.5metres, and then another which is age that goes from 0 to 80, any small change in the height will make the updates go crazy, so you need a small learning rate\n","* Normalization can make mean 0 and standard deviation 1, so can use greater learning rate, to get to minimum faster\n","\n","Decresed importance of weight initilization:\n","*becasue of the above stuf, the graph is alot smoother so you can sample your starting points between -1 and 1, also then youll reach the minimum\n","\n","\n","- also has a tiny regularization effect"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":344,"status":"ok","timestamp":1696016680537,"user":{"displayName":"tj toor","userId":"12772828922137278049"},"user_tz":240},"id":"Tb4lIGyUZUIn"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.datasets import cifar10"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17109,"status":"ok","timestamp":1696016698992,"user":{"displayName":"tj toor","userId":"12772828922137278049"},"user_tz":240},"id":"kq59zqy3adUP","outputId":"5cd3ead2-88f6-4a45-bd3d-73ee2f38be8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170498071/170498071 [==============================] - 14s 0us/step\n"]}],"source":["#Load Data\n","(xtrain,ytrain),(xtest,ytest)=cifar10.load_data()\n","\n","# convert the data to float and also normalize to between 1 and 0\n","xtrain= xtrain.astype(\"float32\")/255.0\n","xtest=xtest.astype(\"float32\")/255.0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1696014305859,"user":{"displayName":"tj toor","userId":"12772828922137278049"},"user_tz":240},"id":"YeEgP2LgdjWd","outputId":"e5142cb8-5cf5-49f8-cd6b-dfe1469cf9c2"},"outputs":[{"data":{"text/plain":["(50000, 32, 32, 3)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["xtrain.shape"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3092,"status":"ok","timestamp":1696016702079,"user":{"displayName":"tj toor","userId":"12772828922137278049"},"user_tz":240},"id":"325qix0NbAdW","outputId":"92c84301-37b3-497a-9dec-a23c1495c30f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 32, 32, 32)        896       \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 16, 16, 32)        0         \n"," D)                                                              \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 16, 16, 64)        18496     \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 8, 8, 64)          0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 8, 8, 64)          36928     \n","                                                                 \n"," flatten (Flatten)           (None, 4096)              0         \n","                                                                 \n"," dense (Dense)               (None, 64)                262208    \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 319178 (1.22 MB)\n","Trainable params: 319178 (1.22 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# Create Model, no need to flatten data because you are using a CNN\n","\n","model = keras.Sequential(\n","    [\n","    keras.Input(shape=(32,32,3)),\n","    layers.Conv2D(32,3,1,padding='same',activation='relu',input_shape=(32,3,3)),\n","    layers.MaxPooling2D(2),\n","    layers.Conv2D(64,3,1,padding='same',activation='relu'),\n","    layers.MaxPooling2D(2),\n","    layers.Conv2D(64,3,1,padding='same',activation='relu'),\n","\n","    # now these layers have to go into the dense layers, ie. the fully connected part\n","    layers.Flatten(),\n","    layers.Dense(64,activation='relu'),\n","    layers.Dense(10)\n","\n","    ]\n","\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"y3Uq3gO2gQa6","outputId":"85551b26-3c7e-46a5-b247-79cf66714ec8"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","391/391 - 13s - loss: 1.5502 - accuracy: 0.4320 - 13s/epoch - 33ms/step\n","Epoch 2/10\n","391/391 - 1s - loss: 1.1643 - accuracy: 0.5887 - 1s/epoch - 4ms/step\n","Epoch 3/10\n","391/391 - 1s - loss: 0.9938 - accuracy: 0.6509 - 1s/epoch - 4ms/step\n","Epoch 4/10\n","391/391 - 1s - loss: 0.8724 - accuracy: 0.6958 - 1s/epoch - 4ms/step\n","Epoch 5/10\n","391/391 - 2s - loss: 0.7919 - accuracy: 0.7243 - 2s/epoch - 4ms/step\n","Epoch 6/10\n","391/391 - 1s - loss: 0.7299 - accuracy: 0.7463 - 1s/epoch - 4ms/step\n","Epoch 7/10\n","391/391 - 2s - loss: 0.6726 - accuracy: 0.7666 - 2s/epoch - 5ms/step\n","Epoch 8/10\n","391/391 - 2s - loss: 0.6297 - accuracy: 0.7803 - 2s/epoch - 4ms/step\n","Epoch 9/10\n","391/391 - 1s - loss: 0.5814 - accuracy: 0.7972 - 1s/epoch - 4ms/step\n","Epoch 10/10\n","391/391 - 1s - loss: 0.5380 - accuracy: 0.8125 - 1s/epoch - 4ms/step\n","79/79 - 0s - loss: 0.7820 - accuracy: 0.7398 - 375ms/epoch - 5ms/step\n"]},{"data":{"text/plain":["[0.782039999961853, 0.739799976348877]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# compile model\n","\n","model.compile(\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=keras.optimizers.Adam(lr=3e-4),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.fit(xtrain,ytrain,batch_size=128,epochs=10,verbose=2)\n","model.evaluate(xtest,ytest,batch_size=128,verbose=2)"]},{"cell_type":"markdown","source":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n","Epoch 1/10\n","391/391 - 13s - loss: 1.5502 - accuracy: 0.4320 - 13s/epoch - 33ms/step\n","Epoch 2/10\n","391/391 - 1s - loss: 1.1643 - accuracy: 0.5887 - 1s/epoch - 4ms/step\n","Epoch 3/10\n","391/391 - 1s - loss: 0.9938 - accuracy: 0.6509 - 1s/epoch - 4ms/step\n","Epoch 4/10\n","391/391 - 1s - loss: 0.8724 - accuracy: 0.6958 - 1s/epoch - 4ms/step\n","Epoch 5/10\n","391/391 - 2s - loss: 0.7919 - accuracy: 0.7243 - 2s/epoch - 4ms/step\n","Epoch 6/10\n","391/391 - 1s - loss: 0.7299 - accuracy: 0.7463 - 1s/epoch - 4ms/step\n","Epoch 7/10\n","391/391 - 2s - loss: 0.6726 - accuracy: 0.7666 - 2s/epoch - 5ms/step\n","Epoch 8/10\n","391/391 - 2s - loss: 0.6297 - accuracy: 0.7803 - 2s/epoch - 4ms/step\n","Epoch 9/10\n","391/391 - 1s - loss: 0.5814 - accuracy: 0.7972 - 1s/epoch - 4ms/step\n","Epoch 10/10\n","391/391 - 1s - loss: 0.5380 - accuracy: 0.8125 - 1s/epoch - 4ms/step\n","79/79 - 0s - loss: 0.7820 - accuracy: 0.7398 - 375ms/epoch - 5ms/step\n","[0.782039999961853, 0.739799976348877]\n","\n","\n","If you look athe above, then you can see the accuracy isnt good\n","\n","this is because we are overfitting  - so you can add regularization to fix it"],"metadata":{"id":"UWncE8f1dCab"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8gAiQQiOjtRk","outputId":"e8470266-f65d-4625-adb9-e277a7734242"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","782/782 - 7s - loss: 1.5502 - accuracy: 0.4435 - 7s/epoch - 9ms/step\n","Epoch 2/10\n","782/782 - 3s - loss: 1.0682 - accuracy: 0.6157 - 3s/epoch - 4ms/step\n","Epoch 3/10\n","782/782 - 4s - loss: 0.8901 - accuracy: 0.6808 - 4s/epoch - 5ms/step\n","Epoch 4/10\n","782/782 - 4s - loss: 0.7796 - accuracy: 0.7249 - 4s/epoch - 5ms/step\n","Epoch 5/10\n","782/782 - 3s - loss: 0.6962 - accuracy: 0.7525 - 3s/epoch - 4ms/step\n","Epoch 6/10\n","782/782 - 3s - loss: 0.6190 - accuracy: 0.7810 - 3s/epoch - 4ms/step\n","Epoch 7/10\n","782/782 - 4s - loss: 0.5487 - accuracy: 0.8053 - 4s/epoch - 5ms/step\n","Epoch 8/10\n","782/782 - 3s - loss: 0.4879 - accuracy: 0.8284 - 3s/epoch - 4ms/step\n","Epoch 9/10\n","782/782 - 3s - loss: 0.4248 - accuracy: 0.8498 - 3s/epoch - 4ms/step\n","Epoch 10/10\n","782/782 - 4s - loss: 0.3703 - accuracy: 0.8688 - 4s/epoch - 5ms/step\n","157/157 - 1s - loss: 0.9911 - accuracy: 0.7077 - 533ms/epoch - 3ms/step\n"]},{"data":{"text/plain":["[0.9911022186279297, 0.7077000141143799]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# build CNN using functional api\n","inputs=keras.Input(shape=(32,32,3))\n","#you dont need to add activation below, becasue we are doing batch normilization, so we do activation after it\n","x=layers.Conv2D(32,3)(inputs)\n","x=layers.BatchNormalization()(x)\n","x=keras.activations.relu(x)\n","#if you dont specify anything, it defaults to a 2x2\n","x=layers.MaxPooling2D()(x)\n","x=layers.Conv2D(64,5,padding='same')(x)\n","x=layers.BatchNormalization()(x)\n","x=keras.activations.relu(x)\n","x=layers.Conv2D(128,3)(x)\n","x=layers.BatchNormalization()(x)\n","x=keras.activations.relu(x)\n","x=layers.Flatten()(x)\n","x=layers.Dense(64,activation='relu')(x)\n","outputs=layers.Dense(10,activation='softmax')(x)\n","model=keras.Model(inputs=inputs,outputs=outputs)\n","\n","model.compile(\n","    loss=keras.losses.SparseCategoricalCrossentropy(),\n","    optimizer=keras.optimizers.Adam(lr=3e-4),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.fit(xtrain,ytrain,batch_size=64,epochs=10,verbose=2)\n","model.evaluate(xtest,ytest,batch_size=64,verbose=2)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gdkvpw2rmHrs","executionInfo":{"status":"ok","timestamp":1696018067474,"user_tz":240,"elapsed":807120,"user":{"displayName":"tj toor","userId":"12772828922137278049"}},"outputId":"e0130b59-b07d-469d-ac57-0b38c1ebd51d"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n","782/782 - 17s - loss: 2.5345 - accuracy: 0.1309 - 17s/epoch - 22ms/step\n","Epoch 2/150\n","782/782 - 5s - loss: 2.2160 - accuracy: 0.1396 - 5s/epoch - 6ms/step\n","Epoch 3/150\n","782/782 - 5s - loss: 2.2016 - accuracy: 0.1425 - 5s/epoch - 6ms/step\n","Epoch 4/150\n","782/782 - 5s - loss: 2.2053 - accuracy: 0.1409 - 5s/epoch - 6ms/step\n","Epoch 5/150\n","782/782 - 5s - loss: 2.1982 - accuracy: 0.1423 - 5s/epoch - 6ms/step\n","Epoch 6/150\n","782/782 - 5s - loss: 2.1948 - accuracy: 0.1460 - 5s/epoch - 6ms/step\n","Epoch 7/150\n","782/782 - 5s - loss: 2.1899 - accuracy: 0.1463 - 5s/epoch - 6ms/step\n","Epoch 8/150\n","782/782 - 5s - loss: 2.1909 - accuracy: 0.1455 - 5s/epoch - 6ms/step\n","Epoch 9/150\n","782/782 - 5s - loss: 2.1796 - accuracy: 0.1470 - 5s/epoch - 6ms/step\n","Epoch 10/150\n","782/782 - 5s - loss: 2.1838 - accuracy: 0.1462 - 5s/epoch - 6ms/step\n","Epoch 11/150\n","782/782 - 5s - loss: 2.1767 - accuracy: 0.1479 - 5s/epoch - 6ms/step\n","Epoch 12/150\n","782/782 - 5s - loss: 2.1689 - accuracy: 0.1502 - 5s/epoch - 7ms/step\n","Epoch 13/150\n","782/782 - 5s - loss: 2.1664 - accuracy: 0.1509 - 5s/epoch - 6ms/step\n","Epoch 14/150\n","782/782 - 5s - loss: 2.1631 - accuracy: 0.1523 - 5s/epoch - 6ms/step\n","Epoch 15/150\n","782/782 - 5s - loss: 2.1575 - accuracy: 0.1553 - 5s/epoch - 6ms/step\n","Epoch 16/150\n","782/782 - 5s - loss: 2.1612 - accuracy: 0.1510 - 5s/epoch - 6ms/step\n","Epoch 17/150\n","782/782 - 5s - loss: 2.1607 - accuracy: 0.1529 - 5s/epoch - 6ms/step\n","Epoch 18/150\n","782/782 - 5s - loss: 2.1593 - accuracy: 0.1508 - 5s/epoch - 6ms/step\n","Epoch 19/150\n","782/782 - 5s - loss: 2.1557 - accuracy: 0.1561 - 5s/epoch - 6ms/step\n","Epoch 20/150\n","782/782 - 5s - loss: 2.1533 - accuracy: 0.1532 - 5s/epoch - 6ms/step\n","Epoch 21/150\n","782/782 - 5s - loss: 2.1527 - accuracy: 0.1541 - 5s/epoch - 6ms/step\n","Epoch 22/150\n","782/782 - 5s - loss: 2.1543 - accuracy: 0.1535 - 5s/epoch - 6ms/step\n","Epoch 23/150\n","782/782 - 5s - loss: 2.1534 - accuracy: 0.1558 - 5s/epoch - 6ms/step\n","Epoch 24/150\n","782/782 - 5s - loss: 2.1517 - accuracy: 0.1553 - 5s/epoch - 6ms/step\n","Epoch 25/150\n","782/782 - 5s - loss: 2.1442 - accuracy: 0.1576 - 5s/epoch - 6ms/step\n","Epoch 26/150\n","782/782 - 5s - loss: 2.0593 - accuracy: 0.1887 - 5s/epoch - 6ms/step\n","Epoch 27/150\n","782/782 - 5s - loss: 2.0383 - accuracy: 0.1940 - 5s/epoch - 6ms/step\n","Epoch 28/150\n","782/782 - 5s - loss: 2.0384 - accuracy: 0.1984 - 5s/epoch - 6ms/step\n","Epoch 29/150\n","782/782 - 5s - loss: 2.0338 - accuracy: 0.1987 - 5s/epoch - 6ms/step\n","Epoch 30/150\n","782/782 - 5s - loss: 2.0310 - accuracy: 0.2002 - 5s/epoch - 6ms/step\n","Epoch 31/150\n","782/782 - 5s - loss: 2.0298 - accuracy: 0.2023 - 5s/epoch - 6ms/step\n","Epoch 32/150\n","782/782 - 5s - loss: 2.0323 - accuracy: 0.2019 - 5s/epoch - 7ms/step\n","Epoch 33/150\n","782/782 - 5s - loss: 2.0285 - accuracy: 0.2021 - 5s/epoch - 6ms/step\n","Epoch 34/150\n","782/782 - 5s - loss: 2.0286 - accuracy: 0.2056 - 5s/epoch - 6ms/step\n","Epoch 35/150\n","782/782 - 5s - loss: 2.0312 - accuracy: 0.2034 - 5s/epoch - 6ms/step\n","Epoch 36/150\n","782/782 - 5s - loss: 2.0279 - accuracy: 0.2051 - 5s/epoch - 6ms/step\n","Epoch 37/150\n","782/782 - 5s - loss: 2.0308 - accuracy: 0.2027 - 5s/epoch - 6ms/step\n","Epoch 38/150\n","782/782 - 5s - loss: 2.0239 - accuracy: 0.2056 - 5s/epoch - 6ms/step\n","Epoch 39/150\n","782/782 - 5s - loss: 2.0098 - accuracy: 0.2095 - 5s/epoch - 6ms/step\n","Epoch 40/150\n","782/782 - 5s - loss: 1.9598 - accuracy: 0.2263 - 5s/epoch - 6ms/step\n","Epoch 41/150\n","782/782 - 5s - loss: 1.9506 - accuracy: 0.2279 - 5s/epoch - 6ms/step\n","Epoch 42/150\n","782/782 - 5s - loss: 1.9434 - accuracy: 0.2298 - 5s/epoch - 6ms/step\n","Epoch 43/150\n","782/782 - 5s - loss: 1.9369 - accuracy: 0.2325 - 5s/epoch - 6ms/step\n","Epoch 44/150\n","782/782 - 5s - loss: 1.9400 - accuracy: 0.2331 - 5s/epoch - 6ms/step\n","Epoch 45/150\n","782/782 - 5s - loss: 1.9361 - accuracy: 0.2337 - 5s/epoch - 6ms/step\n","Epoch 46/150\n","782/782 - 5s - loss: 1.9376 - accuracy: 0.2323 - 5s/epoch - 6ms/step\n","Epoch 47/150\n","782/782 - 5s - loss: 1.9390 - accuracy: 0.2338 - 5s/epoch - 6ms/step\n","Epoch 48/150\n","782/782 - 5s - loss: 1.9264 - accuracy: 0.2354 - 5s/epoch - 6ms/step\n","Epoch 49/150\n","782/782 - 5s - loss: 1.9351 - accuracy: 0.2351 - 5s/epoch - 6ms/step\n","Epoch 50/150\n","782/782 - 5s - loss: 1.9329 - accuracy: 0.2350 - 5s/epoch - 6ms/step\n","Epoch 51/150\n","782/782 - 5s - loss: 1.9287 - accuracy: 0.2375 - 5s/epoch - 6ms/step\n","Epoch 52/150\n","782/782 - 5s - loss: 1.9266 - accuracy: 0.2417 - 5s/epoch - 6ms/step\n","Epoch 53/150\n","782/782 - 5s - loss: 1.9238 - accuracy: 0.2382 - 5s/epoch - 6ms/step\n","Epoch 54/150\n","782/782 - 5s - loss: 1.9290 - accuracy: 0.2369 - 5s/epoch - 6ms/step\n","Epoch 55/150\n","782/782 - 5s - loss: 1.9298 - accuracy: 0.2393 - 5s/epoch - 6ms/step\n","Epoch 56/150\n","782/782 - 5s - loss: 1.9247 - accuracy: 0.2408 - 5s/epoch - 6ms/step\n","Epoch 57/150\n","782/782 - 5s - loss: 1.9273 - accuracy: 0.2371 - 5s/epoch - 6ms/step\n","Epoch 58/150\n","782/782 - 5s - loss: 1.9243 - accuracy: 0.2413 - 5s/epoch - 6ms/step\n","Epoch 59/150\n","782/782 - 5s - loss: 1.9248 - accuracy: 0.2404 - 5s/epoch - 6ms/step\n","Epoch 60/150\n","782/782 - 5s - loss: 1.9237 - accuracy: 0.2395 - 5s/epoch - 6ms/step\n","Epoch 61/150\n","782/782 - 5s - loss: 1.9245 - accuracy: 0.2390 - 5s/epoch - 6ms/step\n","Epoch 62/150\n","782/782 - 5s - loss: 1.9179 - accuracy: 0.2415 - 5s/epoch - 6ms/step\n","Epoch 63/150\n","782/782 - 5s - loss: 1.9203 - accuracy: 0.2413 - 5s/epoch - 6ms/step\n","Epoch 64/150\n","782/782 - 5s - loss: 1.9177 - accuracy: 0.2400 - 5s/epoch - 7ms/step\n","Epoch 65/150\n","782/782 - 5s - loss: 1.9138 - accuracy: 0.2438 - 5s/epoch - 6ms/step\n","Epoch 66/150\n","782/782 - 5s - loss: 1.9192 - accuracy: 0.2415 - 5s/epoch - 6ms/step\n","Epoch 67/150\n","782/782 - 5s - loss: 1.9156 - accuracy: 0.2416 - 5s/epoch - 6ms/step\n","Epoch 68/150\n","782/782 - 5s - loss: 1.9153 - accuracy: 0.2428 - 5s/epoch - 6ms/step\n","Epoch 69/150\n","782/782 - 5s - loss: 1.9212 - accuracy: 0.2421 - 5s/epoch - 6ms/step\n","Epoch 70/150\n","782/782 - 5s - loss: 1.9119 - accuracy: 0.2412 - 5s/epoch - 6ms/step\n","Epoch 71/150\n","782/782 - 5s - loss: 1.9225 - accuracy: 0.2423 - 5s/epoch - 6ms/step\n","Epoch 72/150\n","782/782 - 5s - loss: 1.9155 - accuracy: 0.2446 - 5s/epoch - 6ms/step\n","Epoch 73/150\n","782/782 - 5s - loss: 1.9123 - accuracy: 0.2431 - 5s/epoch - 6ms/step\n","Epoch 74/150\n","782/782 - 5s - loss: 1.9178 - accuracy: 0.2443 - 5s/epoch - 6ms/step\n","Epoch 75/150\n","782/782 - 5s - loss: 1.9148 - accuracy: 0.2441 - 5s/epoch - 6ms/step\n","Epoch 76/150\n","782/782 - 5s - loss: 1.9162 - accuracy: 0.2439 - 5s/epoch - 6ms/step\n","Epoch 77/150\n","782/782 - 5s - loss: 1.9169 - accuracy: 0.2450 - 5s/epoch - 6ms/step\n","Epoch 78/150\n","782/782 - 5s - loss: 1.9118 - accuracy: 0.2451 - 5s/epoch - 6ms/step\n","Epoch 79/150\n","782/782 - 5s - loss: 1.9136 - accuracy: 0.2434 - 5s/epoch - 6ms/step\n","Epoch 80/150\n","782/782 - 5s - loss: 1.9140 - accuracy: 0.2422 - 5s/epoch - 6ms/step\n","Epoch 81/150\n","782/782 - 5s - loss: 1.9109 - accuracy: 0.2442 - 5s/epoch - 6ms/step\n","Epoch 82/150\n","782/782 - 5s - loss: 1.9156 - accuracy: 0.2429 - 5s/epoch - 6ms/step\n","Epoch 83/150\n","782/782 - 5s - loss: 1.9057 - accuracy: 0.2438 - 5s/epoch - 6ms/step\n","Epoch 84/150\n","782/782 - 5s - loss: 1.9165 - accuracy: 0.2425 - 5s/epoch - 6ms/step\n","Epoch 85/150\n","782/782 - 5s - loss: 1.8969 - accuracy: 0.2587 - 5s/epoch - 6ms/step\n","Epoch 86/150\n","782/782 - 5s - loss: 1.8779 - accuracy: 0.2656 - 5s/epoch - 6ms/step\n","Epoch 87/150\n","782/782 - 5s - loss: 1.8674 - accuracy: 0.2697 - 5s/epoch - 6ms/step\n","Epoch 88/150\n","782/782 - 5s - loss: 1.8607 - accuracy: 0.2691 - 5s/epoch - 6ms/step\n","Epoch 89/150\n","782/782 - 5s - loss: 1.8627 - accuracy: 0.2686 - 5s/epoch - 6ms/step\n","Epoch 90/150\n","782/782 - 5s - loss: 1.8558 - accuracy: 0.2721 - 5s/epoch - 6ms/step\n","Epoch 91/150\n","782/782 - 5s - loss: 1.8553 - accuracy: 0.2751 - 5s/epoch - 7ms/step\n","Epoch 92/150\n","782/782 - 5s - loss: 1.8600 - accuracy: 0.2699 - 5s/epoch - 6ms/step\n","Epoch 93/150\n","782/782 - 5s - loss: 1.8555 - accuracy: 0.2713 - 5s/epoch - 6ms/step\n","Epoch 94/150\n","782/782 - 5s - loss: 1.8433 - accuracy: 0.2765 - 5s/epoch - 6ms/step\n","Epoch 95/150\n","782/782 - 5s - loss: 1.8118 - accuracy: 0.2983 - 5s/epoch - 6ms/step\n","Epoch 96/150\n","782/782 - 5s - loss: 1.7999 - accuracy: 0.3047 - 5s/epoch - 6ms/step\n","Epoch 97/150\n","782/782 - 5s - loss: 1.7918 - accuracy: 0.3074 - 5s/epoch - 6ms/step\n","Epoch 98/150\n","782/782 - 5s - loss: 1.7925 - accuracy: 0.3078 - 5s/epoch - 6ms/step\n","Epoch 99/150\n","782/782 - 5s - loss: 1.7917 - accuracy: 0.3118 - 5s/epoch - 6ms/step\n","Epoch 100/150\n","782/782 - 5s - loss: 1.7904 - accuracy: 0.3103 - 5s/epoch - 6ms/step\n","Epoch 101/150\n","782/782 - 5s - loss: 1.7810 - accuracy: 0.3127 - 5s/epoch - 7ms/step\n","Epoch 102/150\n","782/782 - 5s - loss: 1.7759 - accuracy: 0.3158 - 5s/epoch - 6ms/step\n","Epoch 103/150\n","782/782 - 5s - loss: 1.7761 - accuracy: 0.3162 - 5s/epoch - 6ms/step\n","Epoch 104/150\n","782/782 - 5s - loss: 1.7714 - accuracy: 0.3186 - 5s/epoch - 6ms/step\n","Epoch 105/150\n","782/782 - 5s - loss: 1.7728 - accuracy: 0.3180 - 5s/epoch - 6ms/step\n","Epoch 106/150\n","782/782 - 5s - loss: 1.7748 - accuracy: 0.3164 - 5s/epoch - 6ms/step\n","Epoch 107/150\n","782/782 - 5s - loss: 1.7720 - accuracy: 0.3202 - 5s/epoch - 6ms/step\n","Epoch 108/150\n","782/782 - 5s - loss: 1.7695 - accuracy: 0.3194 - 5s/epoch - 6ms/step\n","Epoch 109/150\n","782/782 - 5s - loss: 1.7704 - accuracy: 0.3212 - 5s/epoch - 6ms/step\n","Epoch 110/150\n","782/782 - 5s - loss: 1.7655 - accuracy: 0.3209 - 5s/epoch - 6ms/step\n","Epoch 111/150\n","782/782 - 5s - loss: 1.7638 - accuracy: 0.3203 - 5s/epoch - 7ms/step\n","Epoch 112/150\n","782/782 - 5s - loss: 1.7645 - accuracy: 0.3256 - 5s/epoch - 6ms/step\n","Epoch 113/150\n","782/782 - 5s - loss: 1.7650 - accuracy: 0.3227 - 5s/epoch - 6ms/step\n","Epoch 114/150\n","782/782 - 5s - loss: 1.7668 - accuracy: 0.3229 - 5s/epoch - 6ms/step\n","Epoch 115/150\n","782/782 - 5s - loss: 1.7562 - accuracy: 0.3248 - 5s/epoch - 6ms/step\n","Epoch 116/150\n","782/782 - 5s - loss: 1.7649 - accuracy: 0.3224 - 5s/epoch - 6ms/step\n","Epoch 117/150\n","782/782 - 5s - loss: 1.7626 - accuracy: 0.3270 - 5s/epoch - 6ms/step\n","Epoch 118/150\n","782/782 - 5s - loss: 1.7641 - accuracy: 0.3259 - 5s/epoch - 6ms/step\n","Epoch 119/150\n","782/782 - 5s - loss: 1.7570 - accuracy: 0.3248 - 5s/epoch - 6ms/step\n","Epoch 120/150\n","782/782 - 5s - loss: 1.7630 - accuracy: 0.3214 - 5s/epoch - 6ms/step\n","Epoch 121/150\n","782/782 - 5s - loss: 1.7602 - accuracy: 0.3261 - 5s/epoch - 6ms/step\n","Epoch 122/150\n","782/782 - 5s - loss: 1.7590 - accuracy: 0.3250 - 5s/epoch - 6ms/step\n","Epoch 123/150\n","782/782 - 5s - loss: 1.7562 - accuracy: 0.3266 - 5s/epoch - 6ms/step\n","Epoch 124/150\n","782/782 - 5s - loss: 1.7506 - accuracy: 0.3308 - 5s/epoch - 6ms/step\n","Epoch 125/150\n","782/782 - 5s - loss: 1.7517 - accuracy: 0.3305 - 5s/epoch - 6ms/step\n","Epoch 126/150\n","782/782 - 5s - loss: 1.7538 - accuracy: 0.3276 - 5s/epoch - 6ms/step\n","Epoch 127/150\n","782/782 - 5s - loss: 1.7555 - accuracy: 0.3280 - 5s/epoch - 7ms/step\n","Epoch 128/150\n","782/782 - 5s - loss: 1.7569 - accuracy: 0.3278 - 5s/epoch - 6ms/step\n","Epoch 129/150\n","782/782 - 5s - loss: 1.7472 - accuracy: 0.3306 - 5s/epoch - 6ms/step\n","Epoch 130/150\n","782/782 - 5s - loss: 1.7494 - accuracy: 0.3289 - 5s/epoch - 6ms/step\n","Epoch 131/150\n","782/782 - 5s - loss: 1.7439 - accuracy: 0.3332 - 5s/epoch - 6ms/step\n","Epoch 132/150\n","782/782 - 5s - loss: 1.7504 - accuracy: 0.3327 - 5s/epoch - 6ms/step\n","Epoch 133/150\n","782/782 - 5s - loss: 1.7466 - accuracy: 0.3337 - 5s/epoch - 6ms/step\n","Epoch 134/150\n","782/782 - 5s - loss: 1.7502 - accuracy: 0.3300 - 5s/epoch - 6ms/step\n","Epoch 135/150\n","782/782 - 5s - loss: 1.7483 - accuracy: 0.3300 - 5s/epoch - 6ms/step\n","Epoch 136/150\n","782/782 - 5s - loss: 1.7507 - accuracy: 0.3305 - 5s/epoch - 6ms/step\n","Epoch 137/150\n","782/782 - 5s - loss: 1.7441 - accuracy: 0.3338 - 5s/epoch - 6ms/step\n","Epoch 138/150\n","782/782 - 5s - loss: 1.7506 - accuracy: 0.3330 - 5s/epoch - 6ms/step\n","Epoch 139/150\n","782/782 - 5s - loss: 1.7448 - accuracy: 0.3355 - 5s/epoch - 6ms/step\n","Epoch 140/150\n","782/782 - 5s - loss: 1.7484 - accuracy: 0.3341 - 5s/epoch - 6ms/step\n","Epoch 141/150\n","782/782 - 5s - loss: 1.7485 - accuracy: 0.3340 - 5s/epoch - 6ms/step\n","Epoch 142/150\n","782/782 - 5s - loss: 1.7393 - accuracy: 0.3351 - 5s/epoch - 6ms/step\n","Epoch 143/150\n","782/782 - 5s - loss: 1.7431 - accuracy: 0.3350 - 5s/epoch - 7ms/step\n","Epoch 144/150\n","782/782 - 5s - loss: 1.7407 - accuracy: 0.3357 - 5s/epoch - 6ms/step\n","Epoch 145/150\n","782/782 - 5s - loss: 1.7452 - accuracy: 0.3341 - 5s/epoch - 6ms/step\n","Epoch 146/150\n","782/782 - 5s - loss: 1.7451 - accuracy: 0.3331 - 5s/epoch - 6ms/step\n","Epoch 147/150\n","782/782 - 5s - loss: 1.7418 - accuracy: 0.3345 - 5s/epoch - 6ms/step\n","Epoch 148/150\n","782/782 - 5s - loss: 1.7444 - accuracy: 0.3354 - 5s/epoch - 7ms/step\n","Epoch 149/150\n","782/782 - 5s - loss: 1.7392 - accuracy: 0.3365 - 5s/epoch - 6ms/step\n","Epoch 150/150\n","782/782 - 5s - loss: 1.7352 - accuracy: 0.3375 - 5s/epoch - 6ms/step\n","157/157 - 1s - loss: 1.6178 - accuracy: 0.4769 - 966ms/epoch - 6ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["[1.6178300380706787, 0.47690001130104065]"]},"metadata":{},"execution_count":7}],"source":["#can add dropout, data augmentation and L2 regularization to stop overfitting, or early stopage\n","#https://www.youtube.com/playlist?list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc\n","\n","####################\n","from tensorflow.keras import regularizers\n","\n","####################\n","# build CNN using functional api\n","inputs=keras.Input(shape=(32,32,3))\n","#you dont need to add activation below, becasue we are doing batch normilization, so we do activation after it\n","x=layers.Conv2D(32,3,padding='same',kernel_regularizer=regularizers.l2(0.01))(inputs)\n","x=layers.BatchNormalization()(x)\n","x=keras.activations.relu(x)\n","#if you dont specify anything, it defaults to a 2x2\n","x=layers.MaxPooling2D()(x)\n","x=layers.Conv2D(64,3,padding='same',kernel_regularizer=regularizers.l2(0.01))(x)\n","x=layers.BatchNormalization()(x)\n","x=keras.activations.relu(x)\n","x=layers.Conv2D(128,3,padding='same',kernel_regularizer=regularizers.l2(0.01))(x)\n","x=layers.BatchNormalization()(x)\n","x=keras.activations.relu(x)\n","x=layers.Flatten()(x)\n","x=layers.Dense(64,activation='relu')(x)\n","x=layers.Dropout(0.5)(x)\n","outputs=layers.Dense(10,activation='softmax')(x)\n","model=keras.Model(inputs=inputs,outputs=outputs)\n","\n","model.compile(\n","    loss=keras.losses.SparseCategoricalCrossentropy(),\n","    optimizer=keras.optimizers.Adam(lr=3e-4),\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.fit(xtrain,ytrain,batch_size=64,epochs=150,verbose=2)\n","model.evaluate(xtest,ytest,batch_size=64,verbose=2)\n"]},{"cell_type":"code","source":["\n","\n"],"metadata":{"id":"TP0NPJeEeC8g"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyN1P1Vdbduhvtet5Jyc7dSG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}